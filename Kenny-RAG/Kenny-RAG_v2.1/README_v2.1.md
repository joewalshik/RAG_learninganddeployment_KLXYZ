# ğŸ§  Local RAG Chat App â€“ Version 2.1

This is an enhanced version of a fully local Retrieval-Augmented Generation (RAG) application built with **Streamlit**, **LangChain**, **FAISS**, and **Ollama**. It introduces flexible LLM temperature control and an intelligent query rewriting mechanism to improve answer relevance and conversational flow â€” all while keeping your data private and offline.

---

## ğŸš€ Features

- âœ… **Local-first**: No OpenAI key or internet connection required
- âœ… **Modular LangChain** design (not using `RetrievalQA`)
- âœ… **Custom Prompting**: Forces LLM to only answer using retrieved context or say `"I donâ€™t know"`
- âœ… **User-defined LLM Temperature**: Adjustable creativity/randomness via sidebar slider
- âœ… **Query Rewriting**: Rewrites follow-up questions into standalone queries using chat history
- âœ… **Explicit `top_k` Control** for relevant document retrieval
- âœ… **Chunk Metadata Injection**: Adds source and simulated page number
- âœ… **Simple Reranking Layer**: Keyword-based relevance scoring after vector search
- âœ… **Built-in YAML Evaluation Tool**:
  - Uses `test_questions.yaml` to check how many expected keywords are present
- âœ… **Streamlit UI**:
  - Two tabs: ğŸ” Ask questions | ğŸ§ª Evaluate responses
- âœ… **Supports `.txt`, `.pdf`, `.docx` files**
- âœ… **Runs on local models via Ollama** (e.g. `mistral`, `llama2`)
- âœ… **Can be used without uploading documents**

---

## ğŸ“ Project Structure

```
ğŸ“ RAG/
â”œâ”€â”€ rag_streamlit_app.py       # Main Streamlit app (Ask + Evaluate tabs)
â”œâ”€â”€ local_rag_app.py           # CLI version of the app
â”œâ”€â”€ test_questions.yaml        # YAML file for evaluation
â”œâ”€â”€ start_app.bat              # Optional script to launch app on Windows
â”œâ”€â”€ requirements.txt           # Dependency list
â””â”€â”€ README.md                  # You're reading it!
```

---

## âš™ï¸ Setup Instructions

### 1. Create and Activate a Virtual Environment

```bash
# Windows
python -m venv venv
.venv\Scripts\activate

# macOS/Linux
python3 -m venv venv
source venv/bin/activate
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
# Or manually:
pip install langchain streamlit faiss-cpu langchain-community langchain-ollama langchain-huggingface
```

### 3. Start Ollama

```bash
ollama serve
ollama run mistral
```

(You can replace `mistral` with any supported local model like `llama2`, `gemma`, etc.)

### 4. Launch the App

```bash
streamlit run rag_streamlit_app.py
```

---

## ğŸ’¬ How to Use

### ğŸ” Ask Tab

- Enter a question in the input field
- The app rewrites it (if follow-up) and retrieves top chunks from your local docs
- LLM responds based on the retrieved context
- Adjust LLM temperature via the sidebar to control answer creativity
- Source references are shown (file name + chunk index)

### ğŸ§ª Evaluate Tab

- Place a `test_questions.yaml` in your root directory
- YAML format:
  ```yaml
  - question: "What is a KPI?"
    expected_keywords: ["key performance indicator", "metric"]
  ```
- The app scores each answer by checking how many keywords are matched
- Useful for testing retrieval quality and prompt effectiveness

---

## ğŸ”§ Developer Notes

### âœ… Improvements in This Version

- ğŸ”¥ **LLM Temperature**: Control creativity of model outputs
- ğŸ”„ **Query Rewriting**: Improves multi-turn Q&A with context-aware standalone questions

### ğŸ§  Next Potential Enhancements

- âœï¸ **Semantic Reranking** with cross-encoders (e.g. `MiniLM`)
- ğŸ’¬ **Conversational Memory** via LangChain's memory tools
- ğŸ§ª **Evaluation Metrics** beyond keyword matching (e.g. BLEU, cosine similarity)
- ğŸ” **Multi-query expansion** for richer document search

### ğŸ›  Tech Stack

- `streamlit` Â· `langchain-core` Â· `langchain-community`
- `ollama` (local LLM hosting)
- `faiss` (vector similarity search)
- `huggingface` sentence transformers for embedding

---

## ğŸ“¸ Screenshots 

![image](https://github.com/user-attachments/assets/fc137246-c61f-43c2-a545-17732b4f156e)


---

## ğŸ›¡ Privacy First

All documents, embeddings, and model inferences are processed **locally on your device**. No internet access or API calls are made. Perfect for confidential or internal data environments.

---

## ğŸ™Œ Inspired by

- Azure RAG chat prompt strategies
- LangChain composable workflows
- Ollama offline LLM integrations

---

ğŸš€ Now with temperature + query rewriting. Keep testing and scaling!
