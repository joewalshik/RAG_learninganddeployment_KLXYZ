# ğŸ§  Local RAG Chat App

This is a lightweight, local Retrieval-Augmented Generation (RAG) application built using Streamlit, LangChain, FAISS, and Ollama.

It allows users to upload `.txt`, `.pdf`, or `.docx` documents and ask natural language questions based on their contents â€” **without relying on external APIs or cloud services**.

---

## ğŸš€ Features

- âœ… Local-only: No internet connection or OpenAI key required
- âœ… Supports TXT, PDF, and Word documents
- âœ… Powered by Ollama (e.g. Mistral or LLaMA2 running locally)
- âœ… Conversational memory â€” follow-up questions are supported
- âœ… Easy to use Streamlit interface
- âœ… Document upload optional â€” app still accepts general questions

---

## ğŸ“‚ Project Structure

ğŸ“ RAG/
â”œâ”€â”€ rag_streamlit_app.py # Main Streamlit app
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # You're reading it!


---
## ğŸ§ª Setup Instructions

# Install Ollama and pull a model

ollama run mistral 
(You can also use llama2, gemma, or any other supported local model.)


# **Create a virtual environment** (recommended):


python -m venv venv

on window:
.\venv\Scripts\activate 

On macOS/Linux:
source venv/bin/activate

# Install dependencies

pip install -r requirements.txt

# Run the App
streamlit run rag_streamlit_app.py

# How to Use

Upload one or more .txt, .pdf, or .docx files

Ask any question related to your content

If no documents are uploaded, the assistant will still respond â€” but document-specific answers wonâ€™t be available

All data is processed locally and never saved

## Privacy Note

No data is sent to external APIs or services. This tool runs entirely offline and is safe for use with confidential or internal documents.

---
